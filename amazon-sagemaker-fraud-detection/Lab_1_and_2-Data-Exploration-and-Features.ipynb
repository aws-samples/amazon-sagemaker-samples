{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lab1 and Lab2: Data Transformation and Feature Engineering \n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "There are two datasets that make up the inputs to our models. The first is insurance claim data and the other is customer data. This data has been *synthetically generated*.\n",
    "\n",
    "Raw dataset visualization and manipulation will be done using the following libraries: [seaborn](https://seaborn.pydata.org/) and [pandas](https://pandas.pydata.org/). \n",
    "\n",
    "We will then generate features and store them using Amazon SageMaker Feature Store. \n",
    "\n",
    "\n",
    "This diagram summarizes the next steps\n",
    "\n",
    "![data-processing.png](images/notebooks/data-processing.png)\n",
    "\n",
    "1. Data is transformed through Amazon SageMaker Data Wrangler\n",
    "2. Transformed data is explored and visualized. \n",
    "3. Feature engineering on the processed data\n",
    "3. Prepared data is exported to S3\n",
    "4. Import S3 data into Amazon SageMaker Feature Store\n",
    "5. Query Amazon SageMaker Feature Store to generate train/test datasets\n",
    "6. Optionally run Amazon SageMaker Clarify to detect bias in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "\n",
    "---\n",
    "\n",
    "1. [Prerequisites](<#Prerequisites>)\n",
    "1. [Lab 1: Ingest, Transform And Preprocess Data](<#Lab-1:-Ingest,-Transform-And-Preprocess-Data>)\n",
    "    1. [Data exploration](<#Data-exploration>)\n",
    "    1. [Data visualization](<#Data-visualization>)\n",
    "    1. [Prepare data for training and inference](<#Prepare-data-for-training-and-inference>)\n",
    "1. [Lab 2: Feature Engineering](<#Lab-2:-Feature-Engineering>)\n",
    "    1. [Create the Feature Store](#Create-the-Feature-Store)\n",
    "    1. [Explore Feature Store inside SageMaker Studio](<#Explore-Feature-Store-inside-SageMaker-Studio>)\n",
    "    1. [Create Train and Test Datasets](<#Create-Train-and-Test-Datasets>)\n",
    "    1. [Write training and test data to S3](<#Write-training-and-test-data-to-S3>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required third-party libraries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default SageMaker notebook container already has many helpful packages like Pandas, Numpy and Matplotlib. But charts generated through Seaborn are visually appealing and easier to read. \n",
    "\n",
    "The following python modules will be installed in the next cell:\n",
    "1. `seaborn`: for data visualization\n",
    "2. `sagemaker`: Python SDK to call the SageMaker API\n",
    "3. `boto3`: Python SDK to call the AWS API\n",
    "4. `pandas`: Python data manipulation library\n",
    "\n",
    "(if you encounter a warning about pip running as root user - don't worry about it. We're running everything in a container, so it's not going to break anything. You may also incur some incompatible library errors. Re-run cell 1 again to re-install the libraries and to get rid of them) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 1\n",
    "# Update pip  and install seaborn for visualization\n",
    "!python -m pip install -Uq pip\n",
    "!pip install -q seaborn==0.11.1 sagemaker==2.117.0 boto3==1.24.62"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 1: Ingest, Transform And Preprocess Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data exploration\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start by importing the data into the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data you're going to consume today is available in the `data` folder. As we walk through the workshop you'll find that we create another copy of the data for training purposes. We'll put the training data in an S3 bucket. \n",
    "\n",
    "So let's get started by pulling the data into the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "\n",
    "<font color=\"orange\">As mentioned in the workshop, in the interest of time, we're going to use the preprocessed data for visualization and feature engineering. This data has been created after the transformations have run on the raw dataset.</font>\n",
    "\n",
    "<font color=\"cyan\">The transforms have been created using Amazon SageMaker Data Wrangler. If you wish to learn how to create your own transforms check out the Bonus Labs section of the workshop. The Data Wrangler section also teaches you how to create the final transformed datasets `claims_preprocessed.csv` and `customers_preprocessed.csv`</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 2\n",
    "# Importing required libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns  # visualisation\n",
    "import matplotlib.pyplot as plt  # visualisation\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# grab the pastel color set from seaborn\n",
    "colors = sns.color_palette('pastel')\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "# these two variables have the path to the csv files\n",
    "CLAIMS_DATA_CSV = \"./data/claims_preprocessed.csv\"\n",
    "CUSTOMER_DATA_CSV = \"./data/customers_preprocessed.csv\"\n",
    "\n",
    "\n",
    "# pull the data into the notebook by reading it from the folder. Let's create two DataFrames\n",
    "df_claims = pd.read_csv(CLAIMS_DATA_CSV, index_col=0)\n",
    "df_customers = pd.read_csv(CUSTOMER_DATA_CSV, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore your dataset\n",
    "If you have a huge dataset, it's generally not possible to import data directly into the notebook. But you can always slice it and load it. Once loaded you can preview it using the `head()` function on the Pandas DataFrame to understand what the column definitions look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 3\n",
    "\n",
    "# let's check the claims dataset\n",
    "df_claims.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 5\n",
    "\n",
    "# and the customer dataset\n",
    "df_customers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for any missing data\n",
    "\n",
    "As a data-engineer/scientist before you embark on your adventure, you want to see what the data looks like. Let's check for data inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 6\n",
    "\n",
    "# This will print if there are any empty rows in the two datasets\n",
    "print(f\"There are {df_claims.isnull().sum().sum()} empty rows in claims.csv\")\n",
    "print(f\"There are {df_customers.isnull().sum().sum()} empty rows in customer.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine datasets\n",
    "\n",
    "We should also combine the datasets to understand the correlation between the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Pandas for this purpose. A combined dataset csv is provided to us in the data folder. Let's read that in. It is also possible to combine to dataframes using Pandas. But we're going to just read in the pre-combined dataset for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 7\n",
    "\n",
    "# You can also combine the datasets directly using two data frames\n",
    "# df_combined = pd.concat([df_claims, df_customers], axis=1)\n",
    "\n",
    "# read in the combined csv.\n",
    "df_combined = pd.read_csv(\"./data/claims_customer.csv\")\n",
    "\n",
    "# get rid of an unwanted column\n",
    "df_combined = df_combined.loc[:, ~df_combined.columns.str.contains(\"^Unnamed: 0\")]\n",
    "\n",
    "# once again, let's check a small slice of the data to check the columns\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `describe` command to generate some statistics about the dataset. This is a very useful comamnd to see distribution of data. You can also use `describe` on a single column of the Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 8\n",
    "\n",
    "df_combined.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this combined data lets get some more information like\n",
    "\n",
    "* `.nunique()` gives us the unique values in a series e.g. `[1, 3, 5, 5, 7]` has 4 unique values\n",
    "* `.isnull()` gives us a boolean result of whether a value at hand is null or not. We would count the number of unique values and then divide it by the total number of values to get percentage of missing elements\n",
    "* `.value_counts` gives us the frequency of the occuring value. e.g. `[1, 2, 4, 5, 5, 2]` would give us `{1: 1, 2: 2, 4: 1, 5: 2}`. But if you normalize this, it will represent the relative capacity of the value's occurence. Basically how many times the value has occurred divided by the total values. This is a good way to see the variability in the values of this column.\n",
    "* `dytpe` give us the data type of the series. e.g. `int`, `float` etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 9\n",
    "\n",
    "combined_stats = []\n",
    "\n",
    "# loop over all the columns\n",
    "for col in df_combined.columns:\n",
    "    \n",
    "    # append all stats per column to a list\n",
    "    combined_stats.append(\n",
    "        (\n",
    "            col,\n",
    "            df_combined[col].nunique(),\n",
    "            df_combined[col].isnull().sum() * 100 / df_combined.shape[0],\n",
    "            df_combined[col].value_counts(normalize=True, dropna=False).values[0] * 100,\n",
    "            df_combined[col].dtype,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# finally create a new dataframe with all these values\n",
    "stats_df = pd.DataFrame(\n",
    "    combined_stats,\n",
    "    columns=[\"feature\", \"unique_values\", \"percent_missing\", \"percent_largest_category\", \"datatype\"],\n",
    ")\n",
    "\n",
    "# sort these values by largest category column (descending)\n",
    "stats_df.sort_values(\"percent_largest_category\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization\n",
    "\n",
    "---\n",
    "\n",
    "Next - let's understand the various columns (which will eventually become features) in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar plots and Pair plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data distribution of the customer dataset. We notice there are two columns, `gender_male` and `gender_female`. Let's see what the distribution is male to females in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 10\n",
    "\n",
    "# remember value_count gives us the counts of unique values in a column\n",
    "df_customers.customer_gender_female.value_counts(normalize=True).plot.bar(color=colors[:2])\n",
    "plt.xticks([0, 1], [\"Male\", \"Female\"]);\n",
    "plt.ylabel(\"Percentage of customers\")\n",
    "plt.title(\"Distribution of male v/s female customers in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution is skewed more towards male customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next - plot the bar graph of fraudulent claims from the claims dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 11\n",
    "\n",
    "plt.figure(figsize=(6,6), tight_layout=True)\n",
    "df_claims.fraud.value_counts(normalize=True).plot.bar(color=colors[:2])\n",
    "plt.xticks([0, 1], [\"Not Fraud\", \"Fraud\"]);\n",
    "plt.ylabel(\"Percentage of customers\")\n",
    "plt.title(\"Distribution of fraudulent v/s non fraudulent claims\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Majority of the claims aren't fraudulent which is a good sign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the education categories of the customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 12\n",
    "\n",
    "# set size of the plot\n",
    "plt.figure(figsize=(8,5), tight_layout=True)\n",
    "\n",
    "# this prepares the data that needs to be displayed\n",
    "educ = df_customers.customer_education.value_counts(normalize=True, sort=False)\n",
    "\n",
    "# this plots the bar chart\n",
    "plt.bar(educ.index, educ.values, color=colors[:5])\n",
    "plt.title(\"Education distribution of customers\")\n",
    "plt.ylabel(\"Percentage of customers\")\n",
    "plt.xlabel(\"Customer Education Level in years\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Majority of the customers have at least three years of education"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the total claim amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 13\n",
    "\n",
    "plt.figure(figsize=(8,5), tight_layout=True)\n",
    "\n",
    "# this will create a histogram with same sized 30 bins\n",
    "plt.hist(df_claims.total_claim_amount, bins=30, color=sns.color_palette('Set2')[2], linewidth=2)\n",
    "\n",
    "plt.xlabel(\"Total Claim Amount ($)\")\n",
    "plt.ylabel(\"Number of claims\")\n",
    "plt.title(\"Histogram of claim amounts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Majority of the total claim amounts are under \"$25,000\". The higher claim amounts on the x-axis show up in this chart but they have very few numbers of claims against them. Hence, nothing is visible against them on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 13-2 (optional)\n",
    "# You can verify that there are claims with amounts > $75000 by uncommenting the following lines\n",
    "\n",
    "# plt.figure(figsize=(10,6), tight_layout=True)\n",
    "# plt.hist(df_claims.total_claim_amount[lambda x: x >= 100000], color=sns.color_palette('Set2')[2], linewidth=2)\n",
    "# plt.xlabel(\"Total Claim Amount ($)\")\n",
    "# plt.ylabel(\"Number of claims\")\n",
    "# plt.title(\"Histogram of claim amounts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next - plot the number of claims filed in the past year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 14\n",
    "\n",
    "plt.figure(figsize=(8,5), tight_layout=True)\n",
    "df_customers.num_claims_past_year.hist(color=sns.color_palette('magma')[2], linewidth=0.5)\n",
    "plt.title(\"Number of claims in the per year\")\n",
    "plt.xlabel(\"Number of claims per year\")\n",
    "plt.ylabel(\"Number of customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most customers did not file any claims in the previous year, but some filed as many as 7 claims. Even though the chart doesn't show a Y value against claims 3 - 7, the fact that these value show up on the x-axis means there are some customers who did file 2 to 7 claims. \n",
    "\n",
    "To verify this we can filter out claim numbers 0 to 1 and plot only 2 - 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 14-2\n",
    "\n",
    "plt.figure(figsize=(8,5), tight_layout=True)\n",
    "\n",
    "# lambda is a python construct which helps in writing functions inline. In this case we keep values >= 2\n",
    "# This Lambda is not to be confused by AWS Lambda\n",
    "df_customers.num_claims_past_year[lambda x: x >= 2].hist(color=sns.color_palette('Set2')[2], linewidth=0.5,rwidth=0.7)\n",
    "plt.title(\"Number of claims in the per year\")\n",
    "plt.xlabel(\"Number of claims per year\")\n",
    "plt.ylabel(\"Number of customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, a very few number of people filed 2 to 7 claims compared to 4500+ who never a filed a claim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's check how the fraud claims are distributed between the two genders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 15\n",
    "\n",
    "plt.figure(figsize=(8,5), tight_layout=True)\n",
    "df_combined.groupby(\"customer_gender_female\").sum()[\"fraud\"].plot.bar(color=colors[:2], linewidth=2)\n",
    "plt.xticks([0, 1], [\"Male\", \"Female\"])\n",
    "plt.title(\"Fraud distribution by Gender\");\n",
    "plt.xlabel(\"Gender\")\n",
    "plt.ylabel(\"Total number of fraud cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fraudulent claims seem to come disproportionately from male customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 16\n",
    "\n",
    "ax = sns.pairplot(\n",
    "    data=df_customers, \n",
    "    vars=[\"num_insurers_past_5_years\", \"months_as_customer\", \"customer_age\"],\n",
    "    palette='Set2'\n",
    ");\n",
    "ax.fig.set_figheight(8)\n",
    "ax.fig.set_figwidth(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understandably, the `months_as_customer` and `customer_age` are correlated with each other. A younger person has been driving for a shorter time and therefore they have a shorter duration as a customer.\n",
    "\n",
    "We can also see that the `num_insurers_past_5_years` is negatively correlated with `months_as_customer`. If someone frequently jumped around to different insurers, then they probably spent less time as a customer for this insurer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 17\n",
    "\n",
    "plt.figure(figsize=(8,5), tight_layout=True)\n",
    "ax = sns.lineplot(x=\"num_insurers_past_5_years\", y=\"fraud\", data=df_combined, palette=\"Set2\");\n",
    "ax.set(title=\"Correlation between number of insurers and fraud cases\", xlabel=\"Number of insurers in the past 5 years\", ylabel=\"Number of fraud cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"cyan\">Fraud is positively correlated with having a greater number of insurers over the past 5 years. Customers who switched insurers more frequently also had more prevelance of fraud</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's understand the age distribution in our dataset. We're going to rely on a density and a histogram plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 18\n",
    "\n",
    "plt.figure(figsize=(10,6), tight_layout=True)\n",
    "ax = sns.histplot(df_customers, x=\"customer_age\", fill=True, bins=9, kde=True)\n",
    "ax.set(title=\"Distribution of customer age\", xlabel=\"Customer age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The age distribution is somewhat symmetric. We see a higher density of customers between the ages of 35 to 50. This can be further confirmed by the `describe` call on the df_customers dataframe `customer_age` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 18-2\n",
    "\n",
    "df_customers.customer_age.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='cyan'>75% of the users are less than or equal to 56 years old</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box plots\n",
    "\n",
    "*A brief explanation:*\n",
    "Box plots divide the data into sections that each contain approximately 25% of the data in that set. These sections are called Quartiles. It is a way to see dispersion (variability, scatter or spread) of the data along with outliers. Outliers are data points that are numerically far away from rest of the data and are usually not considered for training.\n",
    "\n",
    "The smallest value and largest value are found at the end of the ‘whiskers’ and are useful for providing a visual indicator regarding the spread of scores (e.g. the range).\n",
    "\n",
    "The line at the center of the box plot is the median. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 19\n",
    "\n",
    "plt.figure(figsize=(8,5), tight_layout=True)\n",
    "ax = sns.boxplot(x=df_customers[\"months_as_customer\"], palette='Set2', linewidth=2.5);\n",
    "ax.set(title='Distribution of months as customers', xlabel='Distribution of customers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this box plot we can see there are datapoints outside of the distribution. <font color=\"orange\">These are outliers</font>. The median in this plot shows that the median \"months as customer\" is less than 100 (~8.3y). \n",
    "\n",
    "\n",
    "The majority of the customers have been customers from 50 to 150 months. Again, this can be confirmed if we use the `describe` call on the `df_customers` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 19-2\n",
    "\n",
    "df_customers.months_as_customer.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 20\n",
    "\n",
    "# Creating a correlation matrix of fraud, gender, months as customer, and number of different insurers\n",
    "\n",
    "plt.figure(figsize=(8,8), tight_layout=True)\n",
    "cols = [\n",
    "    \"fraud\",\n",
    "    \"customer_gender_male\",\n",
    "    \"customer_gender_female\",\n",
    "    \"months_as_customer\",\n",
    "    \"num_insurers_past_5_years\",\n",
    "]\n",
    "corr = df_combined[cols].corr()\n",
    "\n",
    "# plot the correlation matrix\n",
    "ax = sns.heatmap(corr, annot=True, cmap=\"Reds\");\n",
    "\n",
    "ax.set_xticklabels(ax.xaxis.get_ticklabels(), fontsize=8, ha=\"right\", rotation=45)\n",
    "ax.set_yticklabels(ax.yaxis.get_ticklabels(), fontsize=8, va=\"center\", rotation=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fraud is correlated with having more insurers in the past 5 years, and negatively correlated with being a customer for a longer period of time. These go hand in hand and means that long time customers are less likely to commit fraud.\n",
    "\n",
    "Other statistical insights:\n",
    "* Negative correlation between `fraud` and female customers and positive correlation between `fraud` and the male gender. This insight was confirmed in our previous section with barcharts. \n",
    "* Looking at these correlations some may look obvious but correlation charts give a good idea of magnitude of the relationship. What that means is it's one thing to instinctively know that if a certain metric increases the other positively correlated metric will increase as well. However, the rate at which it grows also plays an important part in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 21\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# set the background to white so the chart is readable\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# we're going to stack the following columns against each other for correlations\n",
    "corr_list = [\n",
    "    \"customer_age\",\n",
    "    \"months_as_customer\",\n",
    "    \"total_claim_amount\",\n",
    "    \"injury_claim\",\n",
    "    \"vehicle_claim\",\n",
    "    \"incident_severity\",\n",
    "    \"fraud\",\n",
    "]\n",
    "\n",
    "# get the filtered out data frame for these columns only\n",
    "corr_df = df_combined[corr_list]\n",
    "\n",
    "# round down to 2 decimal digits. Higher decimal digits don't make a lot of difference\n",
    "corr = round(corr_df.corr(), 2)\n",
    "\n",
    "fix, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "# create the heatmap\n",
    "ax = sns.heatmap(corr, ax=ax, annot=True, cmap=\"Reds\")\n",
    "\n",
    "ax.set_xticklabels(ax.xaxis.get_ticklabels(), fontsize=10, ha=\"right\", rotation=45)\n",
    "ax.set_yticklabels(ax.yaxis.get_ticklabels(), fontsize=10, va=\"center\", rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Relatively higher positive correlation between `total_claim_amount` and `injury_claim` than `total_claim_amount` and `vehicle_claim`.\n",
    "* `incident_severity` relatively higher in positive correlation with `vehicle_claim` than `injury_claim` - this indicates that the severe incidents very likely saw higher cases of vehicle claims and fewer of injury claims. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for training and inference\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 22\n",
    "\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SageMaker session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boto3 is the AWS Python SDK and we'll be using it exclusively for our API interaction purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Boto3 session stores configuration state and allows you to create service clients and resources. It houses all the IAM information you will need to interact with various services. So let's go ahead and create a session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook and resources can operate in different regions. Therefore, it is important to specify where these regions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 23\n",
    "\n",
    "# Reference that session\n",
    "boto_session = boto3.session.Session()\n",
    "region = boto_session.region_name\n",
    "\n",
    "# create a sagemaker client\n",
    "sagemaker_boto_client = boto_session.client(\"sagemaker\")\n",
    "\n",
    "# then link the two\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session, sagemaker_client=sagemaker_boto_client\n",
    ")\n",
    "\n",
    "# create an s3 client\n",
    "s3_client = boto3.client(\"s3\", region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing is to get the Execution Role for SageMaker - this role houses all the permissions SageMaker will require to setup various components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 24\n",
    "\n",
    "# comment the line below if you want to use a separate role\n",
    "# sagemaker_execution_role_name = \"AmazonSageMaker-ExecutionRole-20210107T234882\"\n",
    "\n",
    "# Get the default role that was created for this domaim\n",
    "try:\n",
    "    sagemaker_role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    sagemaker_role = iam.get_role(RoleName=sagemaker_execution_role_name)[\"Role\"][\"Arn\"]\n",
    "    print(f\"\\n instantiating sagemaker_role with supplied role name : {sagemaker_role}\")\n",
    "\n",
    "# Get temporary access credentials\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporary credentials are required so that we're not hardcoding our IAM credentials into our code. That is a huge issue and often leads to compromised accounts and/or resources. So your application should always rely on STS or Temporary Credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data in the bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We're going to use the default bucket that was setup when we created the SageMaker studio domain/user \n",
    "\n",
    "We're also going to store these variables and prefixes into the notebooks to be used later. We can do this by using the `%store` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 25\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"fraud-detect-demo\"\n",
    "%store bucket\n",
    "%store prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now going to upload the raw dataset to our S3 bucket. The raw dataset is available under the data folder. We're going to upload `claims.csv` and `customers.csv` to our s3 bucket under the prefix `data/raw`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 26\n",
    "\n",
    "# this will upload claims.csv to data/raw in the bucket\n",
    "s3_client.upload_file(Filename=\"./data/claims.csv\", Bucket=bucket, Key=f\"{prefix}/data/raw/claims.csv\")\n",
    "\n",
    "# this will upload customers.csv to data/raw in the bucket\n",
    "s3_client.upload_file(Filename=\"./data/customers.csv\", Bucket=bucket, Key=f\"{prefix}/data/raw/customers.csv\")\n",
    "\n",
    "# this will upload the combined data as well\n",
    "s3_client.upload_file(Filename=\"./data/claims_customer.csv\", Bucket=bucket, Key=f\"{prefix}/data/raw/claims_customer.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we're going to create the paths where we want the training data and testing data will go within the S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 27\n",
    "\n",
    "# ======> Tons of output_paths\n",
    "training_job_output_path = f\"s3://{bucket}/{prefix}/training_jobs\"\n",
    "train_data_uri = f\"s3://{bucket}/{prefix}/data/train/train.csv\"\n",
    "test_data_uri = f\"s3://{bucket}/{prefix}/data/test/test.csv\"\n",
    "\n",
    "# =======> variables used for parameterizing the notebook run\n",
    "train_instance_count = 1\n",
    "train_instance_type = \"ml.m4.xlarge\"\n",
    "\n",
    "predictor_instance_count = 1\n",
    "predictor_instance_type = \"ml.c5.xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets and Feature Types\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw datasets were transformed using Amazon SageMaker Data Wrangler (Hint: check out the bonus labs if you wish to learn more about Data Wrangler). When Data Wrangler encodes a feature as one-hot-encoded feature, it will default to float data types for those resulting features (one feature --> many columns for the one hot encoding). \n",
    "\n",
    "In this case we're manually setting those dtypes from float to int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 28\n",
    "\n",
    "claims_dtypes = {\n",
    "    \"policy_id\": int,\n",
    "    \"incident_severity\": int,\n",
    "    \"num_vehicles_involved\": int,\n",
    "    \"num_injuries\": int,\n",
    "    \"num_witnesses\": int,\n",
    "    \"police_report_available\": int,\n",
    "    \"injury_claim\": float,\n",
    "    \"vehicle_claim\": float,\n",
    "    \"total_claim_amount\": float,\n",
    "    \"incident_month\": int,\n",
    "    \"incident_day\": int,\n",
    "    \"incident_dow\": int,\n",
    "    \"incident_hour\": int,\n",
    "    \"fraud\": int,\n",
    "    \"driver_relationship_self\": int,\n",
    "    \"driver_relationship_na\": int,\n",
    "    \"driver_relationship_spouse\": int,\n",
    "    \"driver_relationship_child\": int,\n",
    "    \"driver_relationship_other\": int,\n",
    "    \"incident_type_collision\": int,\n",
    "    \"incident_type_breakin\": int,\n",
    "    \"incident_type_theft\": int,\n",
    "    \"collision_type_front\": int,\n",
    "    \"collision_type_rear\": int,\n",
    "    \"collision_type_side\": int,\n",
    "    \"collision_type_na\": int,\n",
    "    \"authorities_contacted_police\": int,\n",
    "    \"authorities_contacted_none\": int,\n",
    "    \"authorities_contacted_fire\": int,\n",
    "    \"authorities_contacted_ambulance\": int,\n",
    "    \"event_time\": float,\n",
    "}\n",
    "\n",
    "customers_dtypes = {\n",
    "    \"policy_id\": int,\n",
    "    \"customer_age\": int,\n",
    "    \"customer_education\": int,\n",
    "    \"months_as_customer\": int,\n",
    "    \"policy_deductable\": int,\n",
    "    \"policy_annual_premium\": int,\n",
    "    \"policy_liability\": int,\n",
    "    \"auto_year\": int,\n",
    "    \"num_claims_past_year\": int,\n",
    "    \"num_insurers_past_5_years\": int,\n",
    "    \"customer_gender_male\": int,\n",
    "    \"customer_gender_female\": int,\n",
    "    \"policy_state_ca\": int,\n",
    "    \"policy_state_wa\": int,\n",
    "    \"policy_state_az\": int,\n",
    "    \"policy_state_or\": int,\n",
    "    \"policy_state_nv\": int,\n",
    "    \"policy_state_id\": int,\n",
    "    \"event_time\": float,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations!**\n",
    "\n",
    "<font color=\"orange\">You have successfully completed Lab 1. Please refer to workshop studio for next steps! </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2: Feature Engineering\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the correct dtypes and create new dataframes for both the datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Feature Store\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 29\n",
    "claims_flow_path = \"\"\n",
    "customers_flow_path = \"\"\n",
    "\n",
    "# if the Data Wrangler job was not run, the claims and customers dataframes will be loaded from local copies\n",
    "timestamp = pd.to_datetime(\"now\").timestamp()\n",
    "\n",
    "claims_preprocessed = pd.read_csv(\n",
    "    filepath_or_buffer=\"./data/claims_preprocessed.csv\", dtype=claims_dtypes\n",
    ")\n",
    "\n",
    "# a timestamp column is required by the feature store, so one is added with a current timestamp\n",
    "claims_preprocessed[\"event_time\"] = timestamp\n",
    "\n",
    "customers_preprocessed = pd.read_csv(\n",
    "    filepath_or_buffer=\"./data/customers_preprocessed.csv\", dtype=customers_dtypes\n",
    ")\n",
    "\n",
    "customers_preprocessed[\"event_time\"] = timestamp\n",
    "\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following block will create feature store client from boto3 library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 30\n",
    "featurestore_runtime = boto_session.client(\n",
    "    service_name=\"sagemaker-featurestore-runtime\", region_name=region\n",
    ")\n",
    "\n",
    "feature_store_session = sagemaker.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_boto_client,\n",
    "    sagemaker_featurestore_runtime_client=featurestore_runtime,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the feature groups\n",
    "The datatype for each feature is set by passing a dataframe and inferring the proper datatype. Feature data types can also be set via a config variable, but it will have to match the corresponding Python data type in the Pandas dataframe when it's ingested to the Feature Group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 31\n",
    "claims_fg_name = f\"{prefix}-claims\"\n",
    "customers_fg_name = f\"{prefix}-customers\"\n",
    "%store claims_fg_name\n",
    "%store customers_fg_name\n",
    "\n",
    "claims_feature_group = FeatureGroup(\n",
    "    name=claims_fg_name, sagemaker_session=feature_store_session\n",
    ")\n",
    "\n",
    "customers_feature_group = FeatureGroup(\n",
    "    name=customers_fg_name, sagemaker_session=feature_store_session\n",
    ")\n",
    "\n",
    "claims_feature_group.load_feature_definitions(data_frame=claims_preprocessed)\n",
    "customers_feature_group.load_feature_definitions(data_frame=customers_preprocessed);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the feature groups\n",
    "You must tell the Feature Group which columns in the dataframe correspond to the required record identifier and event time features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 32\n",
    "print(f\"{customers_fg_name} -- {claims_fg_name} are the feature group names in use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to create feature groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 33\n",
    "record_identifier_feature_name = \"policy_id\"\n",
    "event_time_feature_name = \"event_time\"\n",
    "\n",
    "try:\n",
    "    print(f\"\\n Using s3://{bucket}/{prefix}\")\n",
    "    claims_feature_group.create(\n",
    "        s3_uri=f\"s3://{bucket}/{prefix}\",\n",
    "        record_identifier_name=record_identifier_feature_name,\n",
    "        event_time_feature_name=event_time_feature_name,\n",
    "        role_arn=sagemaker_role,\n",
    "        enable_online_store=True,\n",
    "    )\n",
    "    print(f'Create \"claims\" feature group: SUCCESS')\n",
    "except Exception as e:\n",
    "    code = e.response.get(\"Error\").get(\"Code\")\n",
    "    if code == \"ResourceInUse\":\n",
    "        print(f\"Using existing feature group: {claims_fg_name}\")\n",
    "    else:\n",
    "        raise (e)\n",
    "\n",
    "try:\n",
    "    customers_feature_group.create(\n",
    "        s3_uri=f\"s3://{bucket}/{prefix}\",\n",
    "        record_identifier_name=record_identifier_feature_name,\n",
    "        event_time_feature_name=event_time_feature_name,\n",
    "        role_arn=sagemaker_role,\n",
    "        enable_online_store=True,\n",
    "    )\n",
    "    print(f'Create \"customers\" feature group: SUCCESS')\n",
    "except Exception as e:\n",
    "    code = e.response.get(\"Error\").get(\"Code\")\n",
    "    if code == \"ResourceInUse\":\n",
    "        print(f\"Using existing feature group: {customers_fg_name}\")\n",
    "    else:\n",
    "        raise (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait until feature group creation has fully completed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 34\n",
    "def wait_for_feature_group_creation_complete(feature_group):\n",
    "    status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    while status == \"Creating\":\n",
    "        print(\"Waiting for Feature Group Creation\")\n",
    "        time.sleep(5)\n",
    "        status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    if status != \"Created\":\n",
    "        raise RuntimeError(f\"Failed to create feature group {feature_group.name}\")\n",
    "    print(f\"FeatureGroup {feature_group.name} successfully created.\")\n",
    "\n",
    "\n",
    "wait_for_feature_group_creation_complete(feature_group=claims_feature_group)\n",
    "wait_for_feature_group_creation_complete(feature_group=customers_feature_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest records into the Feature Groups\n",
    "After the Feature Groups have been created, we can put data into each store by using the PutRecord API. This API can handle high TPS and is designed to be called by different streams. The data from all of these Put requests is buffered and written to s3 in chunks. The files will be written to the offline store within a few minutes of ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 35\n",
    "if \"claims_table\" in locals():\n",
    "    print(\n",
    "        \"You may have already ingested the data into your Feature Groups. If you'd like to do this again, you can run the ingest methods outside of the 'if/else' statement.\"\n",
    "    )\n",
    "\n",
    "else:\n",
    "    claims_feature_group.ingest(data_frame=claims_preprocessed, max_workers=3, wait=True)\n",
    "\n",
    "    customers_feature_group.ingest(data_frame=customers_preprocessed, max_workers=3, wait=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for offline store data to become available\n",
    "\n",
    "After ingesting data into the Feature Store, we're going to wait until it is available offline. Which means that the data will be available on Amazon S3 (this usually takes 5-8 minutes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 36\n",
    "if \"claims_table\" not in locals():\n",
    "    claims_table = claims_feature_group.describe()[\"OfflineStoreConfig\"][\"DataCatalogConfig\"][\n",
    "        \"TableName\"\n",
    "    ].replace(\"_\", \"-\")\n",
    "if \"customers_table\" not in locals():\n",
    "    customers_table = customers_feature_group.describe()[\"OfflineStoreConfig\"][\"DataCatalogConfig\"][\n",
    "        \"TableName\"\n",
    "    ].replace(\"_\", \"-\")\n",
    "\n",
    "claims_feature_group_s3_prefix = (\n",
    "    f\"{prefix}/{account_id}/sagemaker/{region}/offline-store/{claims_table}/data\"\n",
    ")\n",
    "\n",
    "customers_feature_group_s3_prefix = (\n",
    "    f\"{prefix}/{account_id}/sagemaker/{region}/offline-store/{customers_table}/data\"\n",
    ")\n",
    "print(claims_feature_group_s3_prefix)\n",
    "\n",
    "offline_store_contents = None\n",
    "while offline_store_contents is None:\n",
    "    objects_in_bucket = s3_client.list_objects(\n",
    "        Bucket=bucket, Prefix=customers_feature_group_s3_prefix\n",
    "    )\n",
    "    if \"Contents\" in objects_in_bucket and len(objects_in_bucket[\"Contents\"]) > 1:\n",
    "        offline_store_contents = objects_in_bucket[\"Contents\"]\n",
    "    else:\n",
    "        print(\"Waiting for data in offline store...\")\n",
    "        time.sleep(60)\n",
    "\n",
    "print(\"\\nData available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 37\n",
    "claims_feature_group.describe()[\"OfflineStoreConfig\"][\n",
    "    \"DataCatalogConfig\"\n",
    "], customers_feature_group.describe()[\"OfflineStoreConfig\"][\"DataCatalogConfig\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Feature Store inside SageMaker Studio\n",
    "----\n",
    "\n",
    "After your feature groups have been created, it's possible to visualize it inside SageMaker Studio.\n",
    "\n",
    "Navigate to the top-most icon that looks like \"Home\". Click on Data and then Feature Store\n",
    "\n",
    "<img src=\"images/notebooks/studio-feature-store.png\" width=\"100%\" />\n",
    "\n",
    "On that screen, it's possible to see details about feature groups we've just created and also create a new feature group, by clicking on the button \"Create Feature Group\". \n",
    "\n",
    "We're just going to review feature group details. To do this, click in any of the feature groups in the \"Feature group name\" column and navigate throughout tabs to see more information about each feature group:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train and Test Datasets\n",
    "----\n",
    "\n",
    "Once the data is available in the offline store, it will automatically be cataloged and loaded into an Athena table (this is done by default, but can be turned off). In order to build our training and test datasets, you will submit a SQL query to join the the Claims and Customers tables created in Athena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 38\n",
    "claims_query = claims_feature_group.athena_query()\n",
    "customers_query = customers_feature_group.athena_query()\n",
    "\n",
    "claims_table = claims_query.table_name\n",
    "customers_table = customers_query.table_name\n",
    "database_name = customers_query.database\n",
    "%store claims_table\n",
    "%store customers_table\n",
    "%store database_name\n",
    "\n",
    "feature_columns = list(set(claims_preprocessed.columns) ^ set(customers_preprocessed.columns))\n",
    "feature_columns_string = \", \".join(f'\"{c}\"' for c in feature_columns)\n",
    "feature_columns_string = f'\"{claims_table}\".policy_id as policy_id, ' + feature_columns_string\n",
    "\n",
    "query_string = f\"\"\"\n",
    "SELECT DISTINCT {feature_columns_string}\n",
    "FROM \"{claims_table}\" LEFT JOIN \"{customers_table}\" \n",
    "ON \"{claims_table}\".policy_id = \"{customers_table}\".policy_id\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's run this query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 39\n",
    "claims_query.run(query_string=query_string, output_location=f\"s3://{bucket}/{prefix}/query_results\")\n",
    "claims_query.wait()\n",
    "dataset = claims_query.as_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And export it to a `.csv` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 40\n",
    "dataset.to_csv(\"./data/claims_customer.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column order will be changed (target column should be the first column on dataset) and dataset will be split into training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 41\n",
    "col_order = [\"fraud\"] + list(dataset.drop([\"fraud\", \"policy_id\"], axis=1).columns)\n",
    "\n",
    "train = dataset.sample(frac=0.80, random_state=0)[col_order]\n",
    "test = dataset.drop(train.index)[col_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 42\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write training and test data to S3\n",
    "---\n",
    "\n",
    "Then those prepared features will be exported to `.csv` files: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 43\n",
    "train.to_csv(\"./data/train.csv\", index=False)\n",
    "test.to_csv(\"./data/test.csv\", index=False)\n",
    "dataset.to_csv(\"./data/dataset.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And uploaded to S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 44\n",
    "train_data_uri = f\"s3://{bucket}/{prefix}/data/train/train.csv\"\n",
    "test_data_uri = f\"s3://{bucket}/{prefix}/data/test/test.csv\"\n",
    "\n",
    "s3_client.upload_file(\n",
    "    Filename=\"./data/train.csv\", Bucket=bucket, Key=f\"{prefix}/data/train/train.csv\"\n",
    ")\n",
    "s3_client.upload_file(\n",
    "    Filename=\"./data/test.csv\", Bucket=bucket, Key=f\"{prefix}/data/test/test.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations!**\n",
    "\n",
    "You have successfully completed Lab 1 & 2."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ea104a27ec0bfe4dd75a8041bbdf2f96213994c7eab885a59dc565823523111b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
